---
title: "Building a prediction model on house prices - Prediction"
author: Chiara Castelli
output: github_document
---

Scope of this script is to build 5 different predictive models from the 01_airbnb_datacleaning :

1. OLS  
2. LASSO  
3. Random Forest  
4. GBM Boosting basic tuning
5. GBM Boosting broad tuning

The final section will compare these models and select the best one, which will be validated in the upcoming scripts on two 'live' datasets: MIQ32024 and BGQ32024.The corresponding codes are available in the folder **Part2_Validity** .

```{r setup, include=FALSE}
# Define libraries and options for the cleaning process
knitr::opts_chunk$set(echo = TRUE)

# packages <- c("readr", "tidyverse", "tidyr", "stringr", "stargazer", "Hmisc", "DescTools", "ggplot2", 'extrafont', 'ggpubr', 'fixest','lmtest','sandwich','gbm','stargazer', 'caret','grid','rattle','ranger','pdp')

# Install packages if not already installed
# for (package in packages) {
#   if (!require(package, character.only = TRUE)) {
#     install.packages(package)
#     library(package, character.only = TRUE)
#   }
# }

library(readr)
library(tidyverse)
library(DescTools)
library(ggplot2)
library(extrafont)
library(ggpubr)
library(fixest)
library(lmtest)
library(sandwich)
library(gbm)
library(stargazer)
library(caret)
library(grid)
library(rattle)
library(ranger)
library(pdp)

options(digits = 3)

```


```{r include = FALSE, message = FALSE , warning = FALSE}
# Define folders and source functions

setwd("C:/Users/Castelli/Desktop/WU_exams/CEU_WiSe25/Machine_Learning/Assignment2")

source("C:/Users/Castelli/DataAnalysis3/da_case_studies/ch00-tech-prep/theme_bg.R")
source("C:/Users/Castelli/DataAnalysis3/da_case_studies/ch00-tech-prep/da_helper_functions.R")


```

# Read Database 

```{r include = FALSE, message = FALSE , warning = FALSE}
# read db
# MIQ1_db <- read_rds("MIQ1_db_clean.rds")
MIQ1_db <- read_rds('https://osf.io/2vscu/download')
names(MIQ1_db)
length(unique(MIQ1_db$id)) # unique ID for each observation

```

## Training and test sets

Before moving on, we split the database into training set (70%) and test set (30%).

```{r include = FALSE, message = FALSE , warning = FALSE}
# test size 
0.3* dim(MIQ1_db)[1]

test_set = sample_n(MIQ1_db,6375)
test_id = unique(test_set$id)

training_set = MIQ1_db[!(MIQ1_db$id %in% test_id),] # exclude post ids from test set

```

## Prediction 

### 1. OLS 

We define different model to test as follows:

- M1: basic var
- M2: basic var + host & review feature
- M3: basic var + host & review feature + amenities dummies 
- M4: basic var + host & review feature + amenities dummies + neighbourhood dummies

Reference categories: No Bedrooms - studio flat (Bedroom), Entire property (Room type)

```{r include = F, message = FALSE , warning = FALSE}

# Define variables for regression

# OLS1: basic var
# OLS2: basic var + host & review feature
# OLS3: basic var + host & review feature + amenities dummies 
# OLS4: basic var + host & review feature + amenities dummies + neighbourhood dummies

# basic variables
# ref category bedrooms: 0-studio flat
# ref category room time: entire property
basic_var <- c('accommodates' ,  'bathrooms' ,  'bedrooms_f_1Broom' , 'bedrooms_f_2Broom','bedrooms_f_3Broom' , 'bedrooms_f_4Broom' ,'`bedrooms_f_5+Broom`' , '`room_type_f_Private room`' ,  '`room_type_f_Shared room`' )
# host & review
host_review_var <- c("d_miss_resp_time" ,  "host_resptime_n" ,  "d_miss_superhost" ,  "host_is_superhost" ,  "availability_30" ,  "number_of_reviews" ,  "minimum_nights" ,  "maximum_nights")
vars_ols2 <- c(basic_var, host_review_var)
# amenities dummies 
amenities_var <- names(MIQ1_db)[136:144]
vars_ols3 <- c(basic_var, host_review_var, amenities_var)
# neighbourhood dummies
neigh_var <- names(MIQ1_db)[c(43:129)]
vars_ols4 <- c(basic_var, host_review_var, amenities_var, neigh_var)
                   
# translate into formula objects                   
formula_ols1  <- paste("price_ln ~", paste(basic_var, collapse = " + "))
formula_ols1  <- as.formula(formula_ols1 )

formula_ols2  <- paste("price_ln ~", paste(vars_ols2, collapse = " + "))
formula_ols2  <- as.formula(formula_ols2)

formula_ols3  <- paste("price_ln ~", paste(vars_ols3, collapse = " + "))
formula_ols3  <- as.formula(formula_ols3)

formula_ols4  <- paste("price_ln ~", paste(vars_ols4, collapse = " + "))
formula_ols4  <- as.formula(formula_ols4)


# Fit the linear model - training set
OLS1_train <- lm(formula_ols1 , data = training_set)
OLS2_train <- lm(formula_ols2 , data = training_set)
OLS3_train <- lm(formula_ols3 , data = training_set)
OLS4_train <- lm(formula_ols4 , data = training_set)

# Fit the linear model - test set 
OLS1_test <- lm(formula_ols1 , data = test_set)
OLS2_test <- lm(formula_ols2 , data = test_set)
OLS3_test <- lm(formula_ols3 , data = test_set)
OLS4_test <- lm(formula_ols4 , data = test_set)

```


```{r echo = F, message = FALSE , warning = FALSE}
# Regression results

stargazer_r(list(OLS1_train, OLS2_train, OLS3_train, OLS4_train ), float=F, se = 'robust', digits=2, dep.var.caption = "Dep. var: ln price", keep.stat = c("rsq","n"), no.space = T, omit = "^neighbourhood_cleansed", add.lines = list(c("Neighbourhood", "No", "No", "No", "Yes")))

```

After running the linear regression model under different specification, we can compare the RMSE obtain at each run on the training set:
 
```{r echo = F, message = FALSE , warning = FALSE}

# evaluation of the models
models_train <- c("OLS1_train", "OLS2_train","OLS3_train", "OLS4_train")
models_test <- c("OLS1_test", "OLS2_test","OLS3_test", "OLS4_test")

models <- c(models_train, models_test)
AIC <- c()
BIC <- c()
RMSE <- c()
RSquared <- c()
regr <- c()
k <- c()

# Get for all models
for ( i in 1:length(models)){
  AIC[i] <- AIC(get(models[i]))
  BIC[i] <- BIC(get(models[i]))
  RMSE[i] <- RMSE(predict(get(models[i])), get(models[i])$model$price_ln)
  RSquared[i] <-summary(get(models[i]))$r.squared
  regr[[i]] <- coeftest(get(models[i]), vcov = sandwich)
  k[i] <- get(models[i])$rank -1
}

# create summary table for all models (training and test)
eval <- data.frame(models, k, RSquared, RMSE, BIC)
eval <- eval %>%
  #mutate(models_train = paste0("(",gsub("reg","",models),")")) %>%
  rename(Model = models, "R-squared" = RSquared, "RMSE" = RMSE, "N predictors" = k)

# Evaluation models Part 1 - training set 
eval_train <- eval %>% filter(Model %like% '%train%')

stargazer(eval_train, summary = F, digits=2, float = F, no.space = T, type = 'text')

```

As shown in the above Table, Model 4 has lowest RMSE and BIC, despite the significant number of predictors compared to the other ones.

We can check if the same conclusion is derived in the Test (or Holdout) set, by looking at the following Table:

```{r echo = F, message = FALSE , warning = FALSE}

# RMSE training versus test
evalRMSE_train_test <- cbind(eval[eval$Model %like% '%train',], eval[eval$Model %like% '%test',])
names(evalRMSE_train_test)[2:5] <- paste(names(evalRMSE_train_test)[2:5], '_train', sep ='')
names(evalRMSE_train_test)[6:10] <- paste(names(evalRMSE_train_test)[6:10], '_test', sep ='')

evalRMSE_train_test <- evalRMSE_train_test %>%  select( c(1,3:5,8:10) ) %>% mutate(Model = substr(Model, 1,4)) 

stargazer(evalRMSE_train_test, summary = F, digits=2, float = F, no.space = T, type = 'text')
```
Same results as before, Model 4 provides the best statistics in terms of BIC and RMSE. As a final step in the decision process, let's perform a Cross-Validation (CV) excercise on both training and test set.  

#### Cross-validation 

We first compute CV separately for the training and test set, then compare RMSE across models and sets. 

That is, we start from the training set:

```{r echo = F,  message = FALSE , warning = FALSE}

# set number of folds
k <- 5

set.seed(13505)
cv1 <- train(formula_ols1, training_set, method = "lm", trControl = trainControl(method = "cv", number = k))

set.seed(13505)
cv2 <- train(formula_ols2, training_set, method = "lm", trControl = trainControl(method = "cv", number = k))

set.seed(13505)
cv3 <- train(formula_ols3, training_set, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")

set.seed(13505)
cv4 <- train(formula_ols4, training_set, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")

# calculate average rmse
# Training set 
cv <- c("cv1", "cv2", "cv3", "cv4")
rmse_cv <- c()

for(i in 1:length(cv)){
  rmse_cv[i] <- sqrt((get(cv[i])$resample[[1]][1]^2 +
                       get(cv[i])$resample[[1]][2]^2 +
                       get(cv[i])$resample[[1]][3]^2 +
                       get(cv[i])$resample[[1]][4]^2)/4)
}

# summarize results
cv_mat <- data.frame(rbind(cv1$resample[4], "Avg RMSE"),
           rbind(cv1$resample[1], rmse_cv[1]),
           rbind(cv2$resample[1], rmse_cv[2]),
           rbind(cv3$resample[1], rmse_cv[3]),
           rbind(cv4$resample[1], rmse_cv[4])
           # rbind(cv5$resample[1], rmse_cv[5])
           )

colnames(cv_mat)<-c("CV Train","Model1", "Model2", "Model3", "Model4")

stargazer(cv_mat, summary = F, digits=2, float=F, type = 'text')
      
```

And continue with the test set: 

```{r echo = F,  message = FALSE , warning = FALSE}

set.seed(13505)
cv1_test <- train(formula_ols1, test_set, method = "lm", trControl = trainControl(method = "cv", number = k))

set.seed(13505)
cv2_test <- train(formula_ols2, test_set, method = "lm", trControl = trainControl(method = "cv", number = k))

set.seed(13505)
cv3_test <- train(formula_ols3, test_set, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")

set.seed(13505)
cv4_test <- train(formula_ols4, test_set, method = "lm", trControl = trainControl(method = "cv", number = k), na.action = "na.omit")

# CV
cv_test <- c("cv1_test", "cv2_test", "cv3_test", "cv4_test")
rmse_cv_test <- c()

for(i in 1:length(cv_test)){
  rmse_cv_test[i] <- sqrt((get(cv_test[i])$resample[[1]][1]^2 +
                       get(cv_test[i])$resample[[1]][2]^2 +
                       get(cv_test[i])$resample[[1]][3]^2 +
                       get(cv_test[i])$resample[[1]][4]^2)/4)

}

# summarize results
cv_mat_test <- data.frame(rbind(cv1_test$resample[4], "Avg RMSE"),
           rbind(cv1_test$resample[1], rmse_cv_test[1]),
           rbind(cv2_test$resample[1], rmse_cv_test[2]),
           rbind(cv3_test$resample[1], rmse_cv_test[3]),
           rbind(cv4_test$resample[1], rmse_cv_test[4])
           # rbind(cv5$resample[1], rmse_cv[5])
           )

colnames(cv_mat_test)<-c("CV Test","Model1", "Model2", "Model3", "Model4")

stargazer(cv_mat_test, summary = F, digits=2, float=F, type = 'text')



```

*Training - Test comparison:*

```{r echo = F, message = FALSE , warning = FALSE}
cv_train <- cv_mat[6,]
names(cv_train) <- 'CV'
cv_train[1,1] <- 'Train set'

cv_test <- cv_mat_test[6,]
names(cv_test) <- 'CV'
cv_test[1,1] <- 'Test set' 

cv_table <- rbind(cv_train , cv_test )
names(cv_table) <- c("CV RMSE", "Model1", "Model2", "Model3", "Model4")

stargazer(cv_table , summary = F, digits=2, float=F, type = 'text')

```
In both cases, Model 4 is preferred.
Hence, we can move on and evaluate the prediction made with M4.

#### Prediction

*Prediction interval at 80% and 90% confidence level* (New observation)

``` {r include = F, message = FALSE , warning = FALSE}

# First on the training 
list_col <- names(training_set)[c(43:129,136:144)]

data_predict <- training_set %>% select( price_n , price_ln , accommodates ,  bathrooms , bedrooms_f_1Broom , bedrooms_f_2Broom , bedrooms_f_3Broom , bedrooms_f_4Broom ,`bedrooms_f_5+Broom` ,  `room_type_f_Private room` ,  `room_type_f_Shared room` ,  d_miss_resp_time ,  host_resptime_n ,  d_miss_superhost ,  host_is_superhost ,  availability_30 ,  number_of_reviews ,  minimum_nights , maximum_nights, all_of(list_col) )

# create new observation
 new <- data_predict[1,]
 new[1,] <- NA
 new$price_ln <- NA
 new$host_is_superhost <- FALSE

 # assign specific values to certain columns
 new <- new %>%
   mutate( accommodates = 4, bathrooms = 1, bedrooms_f_1Broom = 1,
          `room_type_f_Private room` = 0, `room_type_f_Shared room` = 0,
          d_miss_resp_time = 0, host_resptime_n = 2, d_miss_superhost = 0,
           availability_30 = 15, number_of_reviews = 15,
          minimum_nights = 1, maximum_nights = 365)

 # Broom dummies
 new[6:9]<-0
 # Set columns 56 to 98 to 0
 new[1, 20:106] <- 0 # neighborhoods
 new[1, 80] <- 1 # assign 1 to a random neighborhood

 new[1, 107:115] <- 0 # dummies amenities
 new[1, c(107,110,113)] <- 1 # assign 1 to random dummies
 # check only price cols are NA
 names(which(colSums(is.na(new[1,])) > 0)) # okay

#data_predict<- rbind(data_predict, new)
class(data_predict$host_is_superhost) <- 'logical'
# Predict lnprice with Model 4 
# Predict price with all predictors 
reg_pred <- lm(formula_ols4, data = data_predict)
summary(reg_pred)
# prediction
data_predict$price_lnPREDICT <- predict(reg_pred, data_predict)
rmse4 <- RMSE(data_predict$price_lnPREDICT,data_predict$price_ln)

# prediction for new observation 
# 80% PI
predln_new <- predict(reg_pred, newdata = new,se.fit = TRUE, interval = "prediction")
predln_new80 <- predict(reg_pred, newdata = new,se.fit = TRUE, interval = "prediction", level=0.80)
predln_new80
lnp2_new <- predln_new$fit[[1]]
# 90% PI
predln_new90 <- predict(reg_pred, newdata = new,se.fit = TRUE, interval = "prediction", level=0.90)
predln_new90

# convert prediction to levels
data_predict$price_PREDICT <- exp(data_predict$price_lnPREDICT)*exp((rmse4^2)/2)
lnp2_new_lev <- exp(lnp2_new)*exp((rmse4^2)/2)

# prediction intervals (log and level)
lnp2_PIlow <- predln_new$fit[2]
lnp2_PIhigh <- predln_new$fit[3]
lnplev_PIlow <- exp(lnp2_PIlow)*exp(rmse4^2/2)
lnplev_PIhigh <- exp(lnp2_PIhigh)*exp(rmse4^2/2)

# 80% PI
lnp2_PIlow80 <- predln_new80$fit[2]
lnp2_PIhigh80 <- predln_new80$fit[3]
lnplev_PIlow80 <- exp(lnp2_PIlow80)*exp(rmse4^2/2)
lnplev_PIhigh80 <- exp(lnp2_PIhigh80)*exp(rmse4^2/2)

# 90% PI
lnp2_PIlow90 <- predln_new90$fit[2]
lnp2_PIhigh90 <- predln_new90$fit[3]
lnplev_PIlow90 <- exp(lnp2_PIlow90)*exp(rmse4^2/2)
lnplev_PIhigh90 <- exp(lnp2_PIhigh90)*exp(rmse4^2/2)



```

``` {r echo = F, message = FALSE , warning = FALSE}

# summary of predictions and PI 80% and 90% version
sum <- matrix( c( lnp2_new, lnp2_PIlow80 ,lnp2_PIhigh80,lnp2_PIlow90 ,lnp2_PIhigh90,
                  lnp2_new_lev, lnplev_PIlow80, lnplev_PIhigh80,lnplev_PIlow90, lnplev_PIhigh90) , nrow = 5  ,ncol = 2)

colnames(sum) <- c('Model in logs','Convertion to level')
rownames(sum) <- c('Predicted', 'PI_low 80%', 'PI_high 80%','PI_low 90%', 'PI_high 90%')

stargazer(sum, type = "text", digits=2)
# higher confidence probability brings greater PI (due to the greater multiplier)

```

Prediction ranges seem quite wide.

*Prediction versus actual (y-yhat))* (test set)

``` {r echo = F, message = FALSE , warning = FALSE}

test_set$predicted <- predict(OLS4_test, newdata = test_set)

# Scatterplot of Actual vs Predicted Prices
ggplot(test_set, aes(x = predicted, y = price_ln)) +
  geom_point(color = "darkblue", alpha = 0.6) +  # Scatter points
  geom_abline(slope = 1, intercept = 0, color = "darkred", linetype = "dashed") +  # Reference Line
  labs(title = "Actual vs Predicted Prices Test set",
       x = "Predicted Prices (ln)",
       y = "Actual Prices (ln)") +
  theme_minimal()+
  theme(
    axis.text.x = element_text(family = "Garamond", face = "bold"),
    axis.title.x = element_text(family = "Garamond"),
    axis.text.y = element_text(family = "Garamond", face = "bold"),
    axis.title.y = element_text(family = "Garamond"),
    plot.title = element_text(family = "Garamond")) 

```

The model seems to be under-predict very high prices (i.e. y axis > 7)

### 2. LASSO 

Moving to the Lasso specification, we can test our best model M4 under OLS and see how many predictors remain significant.

``` {r include = F, message = FALSE , warning = FALSE}

# Use best OLS model 4 
# Set lasso tuning parameters
train_control <- trainControl(method = "cv", number = 4)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))

set.seed(1234)
lasso_model_train <- caret::train(formula_ols4,
                      data = training_set,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                    na.action=na.exclude)

print(lasso_model_train$bestTune$lambda)

lasso_coeffs_train <- coef(lasso_model_train$finalModel, lasso_model_train$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") 


```

``` {r echo = F, message = FALSE , warning = FALSE}

# filter LASSO coeffs > 0
lasso_coeffs_nz_train <-lasso_coeffs_train %>%
  filter(s1!=0)

#print(nrow(lasso_coeffs_nz_train))
stargazer(lasso_coeffs_nz_train ,summary = F, type = 'text')

# Evaluate model. CV error:
lasso_cv_rmse_train <- lasso_model_train$results %>%
  filter(lambda == lasso_model_train$bestTune$lambda) %>%
  dplyr::select(RMSE)

print(paste('Lasso RMSE Training set:' , round(lasso_cv_rmse_train[1, 1], 2), sep = ' ' ))

```

RMSE LASSO M4: 0.56 versus OLS M4:0.51.
NCOEFF LASSO M4: 11 versus OLS M4: 111.

Lasso looses only a bit of goodness in fitting the model, while it shrinks down the nr. of coefficients significantly. Interestingly,  *d_miss_resp_time* appears significant as in OLS M4. This may indicate that there is no randomness in misreporting the response time, so we will check if its significance persists in the other predictive models.

### 3. RANDOM FOREST

Random Forest creates many decision trees during its training phase, where each tree is trained on a random sub sample (i.e. the bootstrap technique). Importantly, instead of using all features, each tree considers only a random subset of features at each split, which is decided by the analyst. In this excercise, our set of parameters will range from 3 to 12, while the node size will range between 20 to 100 observations. 

As for the selection of the predictors, we start from the OLS M3 and M4 specification to be tested in a RF setting, while additionally including some interactions terms not tested in the OLS, to check whether they emerge as significant. Specifically, we focus on interaction *among amenities*, as well as *between property features and location*.

After performing the different RF specifications on the *training set*, we can check the models performance in the below table:

``` {r include = T, message = FALSE , warning = FALSE}

# from ch14 Data Analysis book, let's add additional interactions
# among amenities
X1  <- c("accommodates*bathrooms_f",  "room_type_f*d_sharebath",
         "room_type_f*d_am_petsallowed")
# with boroughs
X2  <- c("room_type_f*neighbourhood_f", "room_type_f*neighbourhood_f",
         "accommodates*neighbourhood_f" )


predictors_1rf <- c(basic_var)
predictors_2rf <- c(basic_var, host_review_var, amenities_var, neigh_var)
predictors_INTERACTrf <- c(basic_var, host_review_var, amenities_var, neigh_var, X1,X2)


```

``` {r include = F, eval = F, message = FALSE , warning = FALSE}

# do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)


# RF1
# set tuning parameters (nr predictors: 9 )
tune_grid <- expand.grid(
  .mtry = c(3,4,5), # m is about the sqrt of number of variables 
  .splitrule = "variance",
  .min.node.size = c(20, 50, 100 ) #
)

# model 1
set.seed(1234)


rf_model_1 <- train(
  formula(paste0("price_ln ~", paste0(predictors_1rf, collapse = " + "))),
  data = training_set,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)


rf_model_1

```

```{r include = F,  eval = F, message = FALSE , warning = FALSE}

# RF2
# set tuning pars (nr predictors 113)
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(20, 50, 100)
)

set.seed(1234)

# first tried on a sub-sample of 5k observations

rf_model_2 <- train(
  formula(paste0("price_ln ~", paste0(predictors_2rf, collapse = " + "))),
  data = training_set,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)


rf_model_2

# for m=12 seems we get the lowest RMSE when nodes are at the lowest size = 20 (RMSE: .496).
# Hence, in the last round, we add an additional node size of 15 to check for eventual overfitting (to be later compared with the results on the test set)

```



```{r include = F,  eval = F, message = FALSE , warning = FALSE}

# RF3 - interaction model
# set tuning pars (nr predictors 119)
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(15, 20, 50, 100)
)

set.seed(1234)

# first tried on a sub-sample of 5k observations

rf_model_3 <- train(
  formula(paste0("price_ln ~", paste0(predictors_INTERACTrf, collapse = " + "))),
  data = training_set,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity"
)


rf_model_3


```


``` {r echo = F,  eval = F, message = FALSE , warning = FALSE}
# evaluate random forests 

results_rf <- resamples(
  list(
    model_1  = rf_model_1,
    model_2  = rf_model_2,
    model_3 = rf_model_3
    
  )
)

summary(results_rf)

```


Where Model 2 (wich corresponds to OLS M4 set of predictors) is chosen as best RF model. Within this specification, the lowest RMSE is found when the nr of parameters is set to 12 and at the lowest node size of 20 (might be indicate the presence of overfitting). 

We can test the goodness of the selected model on our test set by first running the same RF model, and then looking at the resulting RMSE distribution within different predictors categories, as shown in the following Table:

#### Subsample performance on test set: RMSE-to-mean(y) ratio

``` {r echo = F, eval = F, message = FALSE , warning = FALSE}

data_test_w_prediction <- test_set %>%
  mutate(predicted_price = predict(rf_model_2, newdata = test_set))

# create nice summary table of heterogeneity
a <- data_test_w_prediction %>%
  mutate(is_low_size = ifelse(accommodates <= 3, "small apt", "large apt")) %>%
  group_by(is_low_size) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price_ln),
    mean_price = mean(price_ln),
    rmse_norm = RMSE(predicted_price, price_ln) / mean(price_ln)
  )


b <- data_test_w_prediction %>%
  filter(neighbourhood_cleansed %in% c("BRERA", "DUOMO")) %>%
  group_by(neighbourhood_cleansed) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price_ln),
    mean_price = mean(price_ln),
    rmse_norm = rmse / mean_price
  )

c <- data_test_w_prediction %>%
  filter(room_type_f %in% c("Shared room", "Private room")) %>%
  group_by(room_type_f) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price_ln),
    mean_price = mean(price_ln),
    rmse_norm = rmse / mean_price
  )


d <- data_test_w_prediction %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price_ln),
    mean_price = mean(price_ln),
    rmse_norm = RMSE(predicted_price, price_ln) / mean(price_ln)
  )

# Save output
colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")

line1 <- c("Nr people", "", "", "")
line2 <- c("Room Type", "", "", "")
line3 <- c("Borough", "", "", "")

result_3 <- rbind(line1, a,line2 , c, line3, b, d) %>%
  transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
            `RMSE/price` = as.numeric(`RMSE/price`))

stargazer(result_3, type = 'text', summary = F)

```

Here, with the exception of the category 'shared room', the overall stability in the RMSE-to-price ratios indicates a good model performance.

We can now have a look at some graphics to help interpret our RF model.


#### Variable Importance Plots Train Set - Top 11

We can start by looking at the singular most important predictors (i.e. ungrouped categorical variables) to check, for instance, the difference of results with Lasso.

```{r echo = F, eval = F, message = FALSE , warning = FALSE }

rf_model_2_var_imp <- ranger::importance(rf_model_2$finalModel)/1000

rf_model_2_var_imp_df <-
  data.frame(varname = names(rf_model_2_var_imp),imp = rf_model_2_var_imp) %>%
  mutate(varname = gsub("neighbourhood_f", "Location:", varname) ) %>%
  mutate(varname = gsub("room_type_f", "Room type:", varname) ) %>%
  mutate(varname = gsub("bedrooms_f", "Broom nr:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))


# have a version with top 11 vars only
rf_model_2_var_imp_plot_b <-
  
  ggplot(rf_model_2_var_imp_df[1:11,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color=color[1], size= 1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color=color[1], size=0.75) +
  ylab("Importance (Percent) RF") +
  xlab("") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bg() +
  theme(axis.text.x = element_text(family = "Garamond", size= 10), axis.text.y = element_text(family = "Garamond",  size= 10),
        axis.title.x = element_text(family = "Garamond", size= 10), axis.title.y = element_text(family = "Garamond", size= 10))

# Save the plot as an image

ggsave("rf_model_2_var_imp_plot_b.png", plot = rf_model_2_var_imp_plot_b , width = 6, height = 4, dpi = 300)


```


We can notice that these variables corresponds to the Lasso results, with the addition of the information on minimum and maximum days of stay. Furthermore, the two very central neighbourhoods of **Brera** and **Duomo** appear as the most important locations, as together account for ~8%. 

We can further aggregate the categorical variables (i.e. neighbourhood, room type and bedroom/ property size):

``` {r echo = F, eval = F, message = FALSE , warning = FALSE}

# Define function to calculate grouped importance
group_importance <- function(rf_obj, groups) {
  var_imp <- as.matrix(sapply(groups, function(g) {
    sum(importance(rf_obj)[g], na.rm = TRUE)
  }))
  colnames(var_imp) <- "MeanDecreaseGini"
  return(var_imp)
}

# Extract variable names from the model
varnames <- rf_model_2$finalModel$xNames

# Identify variable names by their prefixes
extract_varnames <- function(prefix) grep(prefix, varnames, value = TRUE)

groups <- list(
  neighbourhood_cleansed = extract_varnames("neighbourhood_cleansed_"),
  f_room_type = extract_varnames("`room_type_f_"),
  f_broom_nr = extract_varnames("`bedrooms_f_")
 
)

# Calculate grouped importance
rf_model_2_var_imp_grouped <- group_importance(rf_model_2$finalModel, groups)
rf_model_2_var_imp_grouped_df <- data.frame(
  varname = rownames(rf_model_2_var_imp_grouped),
  imp = rf_model_2_var_imp_grouped[,1]
)


# Get non-factor variable importance
nonfactor_var_imp_df <- data.frame(
  varname = varnames,
  imp = importance(rf_model_2$finalModel)
)

nonfactor_var_imp_df <- nonfactor_var_imp_df[!(nonfactor_var_imp_df$varname %like% c("%neighbourhood%", "%room_type%", "%bedrooms_f%") ) , ]

# Combine and calculate importance percentage
rf_model_2_var_imp_combined_df <- rbind(
  nonfactor_var_imp_df,
  rf_model_2_var_imp_grouped_df
) %>% mutate(imp_percentage = imp / sum(imp))

# Plot the top 12 variables by importance
rf_model_2_var_imp_grouped_plot <- ggplot(
  rf_model_2_var_imp_combined_df %>% top_n(12),
  aes(x = reorder(varname, imp), y = imp_percentage)
) +
  geom_point(color = color[1], size = 1) +
  geom_segment(aes(x = varname, xend = varname, y = 0, yend = imp_percentage), color = color[1], size = 0.7) +
  ylab("Importance (Percent)") + xlab("Grouped Names") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bg() +
  theme(axis.text.x = element_text(family = "Garamond", size= 10), axis.text.y = element_text(family = "Garamond",  size= 10),
        axis.title.x = element_text(family = "Garamond", size= 10), axis.title.y = element_text(family = "Garamond", size= 10))

plot(rf_model_2_var_imp_grouped_plot)

```

An important limitation of this plot lies in the fact that the group importance is given by the direct sum of all individual importance values of each category-dummy. In this way however, the raw sum could underestimate the importance of the overall qualitative variable due to the existing collinearity across dummies. Unfortunately, due to time constraints, the suggested Python code is not translated into R for this assignment so that we should treat these results with caution. Having this acknowledgment in mind, we can still notice that the neighbourhood information seems to play a very important role in Airbnb pricing. 

Moreover, in addition to the visualization of the mostly relevant predictors for our RF model, we still don't know their magnitude. Are the results of OLS and Lasso confirmed by the RF excerice? 

To answer to this question, we can have a look at some Partial Dependence Plots (PDP) plotted in the next section.

#### Partial Dependence Plots Train Set

```  {r echo = F, eval = F, message = FALSE , warning = FALSE}
# Nr accommodates
pdp_n_acc <- pdp::partial(rf_model_2, pred.var = "accommodates", pred.grid = distinct_(test_set, "accommodates"), train = training_set)

pdp_n_acc_plot <- pdp_n_acc %>%
  autoplot( ) +
  geom_point(color=color[1], size=2) +
  geom_line(color=color[1], size=1) +
  ylab("Predicted price (ln)") +
  xlab("Accommodates (nr persons)") +
  scale_x_continuous(limit=c(1,7), breaks=seq(1,7,1))+
  theme_bg() +
  theme(axis.text.x = element_text(family = "Garamond", size= 10), axis.text.y = element_text(family = "Garamond",  size= 10),
        axis.title.x = element_text(family = "Garamond", size= 14), axis.title.y = element_text(family = "Garamond", size= 14))


# number_of_reviews

pdp_n_rev <- pdp::partial(rf_model_2, pred.var = "number_of_reviews", pred.grid = distinct_(test_set, "number_of_reviews"), train = training_set)

pdp_n_rev_plot <- pdp_n_rev %>%
  autoplot( ) +
  geom_point(color=color[1], size=2) +
  geom_line(color=color[1], size=1) +
  ylab("Predicted price (ln)") +
  xlab("Nr reviews") +
  scale_x_continuous(limit=c(1,32), breaks=seq(1,32,1))+
  theme_bg() +
  theme(axis.text.x = element_text(family = "Garamond", size= 10), axis.text.y = element_text(family = "Garamond",  size= 10),
        axis.title.x = element_text(family = "Garamond", size= 14), axis.title.y = element_text(family = "Garamond", size= 14))


plot_p <- ggarrange(pdp_n_acc_plot, pdp_n_rev_plot, nrow = 2)

pp <- annotate_figure(plot_p, top = text_grob("PDP numerical variables, MIQ1", 
                                              color = "black", family = "Garamond", size = 14))


# Save the plot as an image

ggsave("pdp_numeric.png", plot = pp , width = 6, height = 4, dpi = 300)

```

Overall, we can say that both magnitude and functional forms are similar to those included in the OLS.


## 4. BOOSTING 

The last model family we try considers Boosting algorithms. Like in Random Forests, the idea behing boosting is to still build multiple decision trees, but with a key difference: instead of growing trees independently, it builds them sequentially.

This is performed in order to improve the prediction fit at each new tree, by giving more weight to observations that were poorly predicted in the previous round. This iterative process helps improve overall model accuracy and reduces bias, and it is usually preferred among other prediction models together with Random Forest algorithms.

In this application, we start by including all predictors of our preferred RF model, and run the specification in a Boosting model that considers two versions of the Gradient boosting machine (GBM) algorithm, where additional complexity is added in the search of the tuning parameters in the second Boosting model.

### 4.1 GBM basic

``` {r include = F, eval = FALSE, message = FALSE , warning = FALSE}

gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 10), # complexity of the tree
                         n.trees = (4:10)*50, # number of iterations, i.e. trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)

set.seed(1234)
  gbm_model <- train(formula(paste0("price_ln ~", paste0(predictors_2rf, collapse = " + "))),
                     data = training_set,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)

gbm_model

```

### 4.2 GBM broad

```{r include = F, eval = FALSE, message = FALSE , warning = FALSE}
# the next will be in final model, loads of tuning
 gbm_grid2 <-  expand.grid(interaction.depth = c(1, 3, 5, 7, 9), # complexity of the tree
                           n.trees = (4:10)*50, # number of iterations, i.e. trees
                           shrinkage = c(0.05, 0.1, 0.15, 0.2), # learning rate: how quickly the algorithm adapts
                           n.minobsinnode = c(5,10,20) # the minimum number of training set samples in a node to commence splitting
)


set.seed(1234)

gbm_model2 <- train(formula(paste0("price_ln ~", paste0(predictors_2rf, collapse = " + "))),
                      data = training_set,
                      method = "gbm",
                      trControl = train_control,
                      verbose = FALSE,
                      tuneGrid = gbm_grid2)

gbm_model2

```

## RF versus GBM most relevant features

We can compare the relevance of the predictors by looking at the Importance Plot under RF and GBM broad boosting models. As shown in the plot below, the set of top 11 predictors is identical, with minor changes in the relative importances.

This shows a convergence in the identification of the most relevant variable for our price analysis. We can conclude by having a look at the horse race table in the following section.

```{r echo = F, eval = F, message = FALSE , warning = FALSE}

gbm_imp_df <- as.data.frame(summary(gbm_model2$finalModel))
gbm_imp_df <- gbm_imp_df[order(gbm_imp_df$rel.inf , decreasing = T),]
gbm_imp_df$rel.inf <- gbm_imp_df$rel.inf / 100
  
# have a version with top 12 vars only
gbm_plot<- ggplot(gbm_imp_df[1:11,], aes(x=reorder(var, rel.inf), y= rel.inf )) +
  geom_point(color=color[1], size= 1) +
  geom_segment(aes(x=var,xend=var ,y=0,yend=rel.inf), color=color[1], size=0.75) +
  ylab("Importance (Percent) GBM") +
  xlab(" ") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bg() +
  theme(axis.text.x = element_text(family = "Garamond", size= 10), axis.text.y = element_text(family = "Garamond",  size= 10),
        axis.title.x = element_text(family = "Garamond", size= 10), axis.title.y = element_text(family = "Garamond", size= 10))


gg<- ggarrange(rf_model_2_var_imp_plot_b , gbm_plot , nrow = 2)


# Save the plot as an image

ggsave("rf_gbm_compare.png", plot = gg , width = 6, height = 4, dpi = 300)

```


## HORSES RACE: Models comparison and final discussion

The final decision on which model to choose is done by looking at the comparison across RMSE computed under different model specifications.

In particular, the following table shows a comparison across model performance considering the training set:

```{r echo = F,  eval = F, message = FALSE , warning = FALSE}

final_models <-  list( #"OLS M4" = OLS4_train,
  #"LASSO M4" = lasso_model_train,
  "Random forest (smaller model M3)" = rf_model_1,
  "Random forest M4" = rf_model_2,
  "Random forest M4 + interact" = rf_model_3,
  "GBM (basic tuning) M4"  = gbm_model,
  "GBM (broad tuning) M4" = gbm_model2)

results <- resamples(final_models) %>% summary()

# Model selection is carried out on this CV RMSE

horsetab_train <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")

horsetab_train_OLS <- eval_train$RMSE[eval_train$Model == 'OLS4_train']
horsetab_train_OLS <- as.data.frame(horsetab_train_OLS)
row.names(horsetab_train_OLS) <- 'OLS M4'
colnames(horsetab_train_OLS) <- "CV RMSE"

horsetab_train_LASSO <- round(lasso_cv_rmse_train[1, 1], 3)
horsetab_train_LASSO <- as.data.frame(horsetab_train_LASSO)
row.names(horsetab_train_LASSO) <- 'LASSO M4'
colnames(horsetab_train_LASSO) <-  "CV RMSE"

horsetab_addon <- rbind(horsetab_train_OLS, horsetab_train_LASSO)

horsetab_train <- rbind(horsetab_addon ,horsetab_train)


stargazer(horsetab_train, summary = F, type = 'html', out = 'horsetab_train.doc')

```


Where the lowest RMSE is given by the Boosting GBM model, with almost no difference between basic a broad tuning models.

Hence, we can look at the corresponding table considering the test set:

```{r echo = F, eval = F, message = FALSE , warning = FALSE}

evaltab_test <- map(final_models, ~{
  RMSE(predict(.x, newdata = test_set), test_set[["price_ln"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

evaltab_test_OLS <- eval$RMSE[eval$Model == 'OLS4_test']
evaltab_test_OLS <- as.data.frame(evaltab_test_OLS)
row.names(evaltab_test_OLS) <- 'OLS M4'
colnames(evaltab_test_OLS) <- "Holdout RMSE"

evaltab_test <- rbind(evaltab_test_OLS , evaltab_test)


stargazer(evaltab_test, summary = F, type = 'text')
stargazer(evaltab_test, summary = F, type = 'html', out = 'evaltab_test.doc')

```

After a second RMSE evaluation on the test set, we select the basic GBM as our workhorse model. Despite considering 111 predictors, a subset of 11 key variables is identified including property characteristics (i.e. guest capacity, property size, number of bathrooms, and Airbnb availability in terms of minimum/maximum days and availability over the next 30 days), the number of reviews, the type of room offered (e.g. entire property vs. single private rooms), and their location, with a premium effect observed for properties in Brera and Duomo.

To validate the selection of this model, we will apply the same model selection routine to the other available datasets - i.e. MIQ3 and BGQ3 - to determine whether both the GBM specification and the identified key variables remain the most relevant predictors for Airbnb property price.  




